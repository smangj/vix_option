% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2".
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}
\usepackage{geometry}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}
\usepackage{booktabs}

% self imported package
\usepackage{rotating}
\usepackage{makecell}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 9 in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Machine Learning Trading Strategy in VIX Derivatives:\\{\textit{A Walk-Forward Training and Backtesting Study}}} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Sangyuan Wang\textsuperscript{1\Yinyang},
Keran Li\textsuperscript{1\Yinyang}
JiaQi Chen \textsuperscript{2,3\textcurrency},
 Liang Xu \textsuperscript{2},
% Name5 Surname\textsuperscript{2\ddag},
% Name6 Surname\textsuperscript{2\ddag},
% Name7 Surname\textsuperscript{1,2,3*},
%with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} Southwestern University of Finance and Economics, Chengdu, Sichuan, China
\\
%\textbf{2} Affiliation Dept/Program/Center, Institution Name, City, State, Country
%\\
%\textbf{3} Affiliation Dept/Program/Center, Institution Name, City, State, Country
%\\
%\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
%
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
\ddag These authors also contributed equally to this work.

% Current address notes
\textcurrency Current Address: Southwestern University of Finance and Economics, Chengdu, Sichuan, China % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address
% \textcurrency c Insert third current address

% Deceased author note
\dag Deceased

% Group/Consortium Author Note
\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* correspondingauthor@institute.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
This study investigates the predictive capabilities of term structures derived from VIX constant-maturity futures (VIX CMFs), expanding from the methodology proposed by Avellaneda (2021). Both predictive and back testing performances are evaluated based on various metrics with a strict walk-forward expanding time windows procedure. The main conclusions are the VIX Cmfs term structure information, specifically \(\mu_t\) and \(\triangle roll\), extracted from \(V_t\) and \(roll_t\), is highly effective in predicting the next-day returns of VIX CMFs. This finding is supported by back-testing analysis, indicating potential economic benefits. However, it was also observed that time-series derivatives have a minimal impact on enhancing predictive accuracy. \textit{(machine learning)}. Another contribution of our work is the development of the Constrained-Mean-Variance Portfolio Optimization (C-MVO) strategy. This strategy demonstrates superior back-testing performance across most models compared to the ordinal-based long-short strategy, offering valuable insights and practical implications for the development of trading strategies for VIX CMFs based on numerical predictions.
% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.
%\section*{Author summary}
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

%\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
The implied volatility index (VIX), often termed as the “investor fear gauge” , has been a pivtoal index in the financial market after its first inception in early 1990s. Since 2003, VIX reflects the 30 days standard \& poor (S \& P 500) expected volatility implied in various call and put S P 500 options (Whaley, 2009; Cboe, 2023). Due to VIX itself is not directly tradable. Instruments such as VIX futures, options, and Exchange-traded notes(ETN) are utilised as essential tools for trading VIX. Meanwhile, With investors growing interests in VIX, S \& P introduced the VIX futures index series in 2005. This series replicates constant-maturity VIX futures indexes by systematically rolling future contracts with varying maturities. Specifically, the VIX short-term futures Index (SPVXSP) keeps a fixed 1-month constant maturity, the VIX two-month futures index (SPVIX2ME) maintains a constant 2-months maturity, three-month (SPVIX3ME), four-month (SPVIX4ME), five-month (SPVIXMP), and six-month (SPVIX6ME) futures indices, each maintain constant maturities corresponding to their respective timeframes. Fig.\ref{fig1} illustrates the correlations among these six constant maturity futures (CMFs).

\begin{figure}[!h]
\caption{\bf CMFs Correlations}
%\includegraphics[width=0.5\linewidth]{EDA_correlations.png}
\label{fig1}
\end{figure}

The VIX term structures describes the relationship of VIX futures contracts with varying expiration dates.The term structure is in contango when VIX futures with longer maturities have higher prices than futures with shorter maturities, backwardation occurs when prices of VIX futures with shorter time to expiration exceed those with longer maturities. Our empirical analysis,  as showcased in exhibit 1 through K-means clustering, robustly indicates a dominant contango shape within the VIX future term structure. Such phenomenon can be partially explained based on volatility risk premium theory (Carr \& Wu, 2008) and  Johnson (2017) work, which divides VIX term strucuture  into two fundamental components: conditional volatility expectations and risk premiums. It becomes apparent that investors stipulate a larger risk premium for further VIX options, as contrasted with the shorter-maturity VIX options except during certain financial crisis. Notably, the VIX futures term structure holds predictive power when anomalies in the term structure revert to the classical shape given strong mean-reverting property of VIX futures illustrated by Avellaneda and Papanicolaou (2019).
\\The literature has extensively explored predictive power of implied volatility (IV),  IV term structure and VIX term structure using classical statistical models. Wang and Wang (2016) among others find that incorporating IV can enhance forecast accuracy within the GARCH model framework. Additionally, studies by Bush et al. (2011), Byun and Kim (2013), and Haugom et al. (2014) have demonstrated the informative power of IV using the heterogeneous autoregressive (HAR) model.transitioning towards term structures, few studies confirms the prediction power of term structures for various financial instruments. Chang (2016) shows the information gained in VIX term structure significantly aid in predicting S\&P 500 index returns volatility while Clements (2020) uses HAR model to further affirm that the information derived from IV term structure possesses exceptional predictive power for forecasting both the level and directional changes in stock return volatility, even during the volatile period of the Global Financial Crisis.Chen (2022) employes CHH model and confirms that the volatility factor constructed from swaptions IV term structures holds significant predictive power on excess bond returns. Lastly, Ornelas and Mauad (2019) provides empirical evidence of the IV term structures informative power in forecasting exchange rate returns. Based on the robust findings from above research use classical statistical models to exploit the predictive power of term structures, in this paper, we use HAR model as the baseline to compare with other selected machine approaches.
\\Despite the booming of machine learning (ML) approaches, relatively scarce studies use machine learning approach to exploit VIX-related issues. Hosker et al (2018) finds that recurrent neural network(RNN) and long short-term memory(LSTM) shows the best performance for forecasting 1-month VIX futures 3-days and 5-days ahead among principal components analysis and ARIMA model. Hirsa and et al. (2021) apply neural network models (random forest, support vector machines, feed-forward neural network and LSTM) to investigate the approaches that could replicate VIX index and VIX futures with fewer options compared to CBOE original methodology. Vrontos (2021) applies various machine learning models and discovers that ML models are statistical and economic superior compared to standard econometric models when forecasting the directional changes of VIX index. Lastly, Avellaneda (2021) generates VIX futures trading signals by including functional dependence of VIX term structures, VIX futures position as well as expected utility in neural network and finds that this approach shows better portfolio performance in the out of sample backtesting.
\\This paper employs a variety of machine learning models, including neural-based network models, tree-based models, and vanilla linear regression model to explore the informative power of VIX CMFs term structure on next-day returns of VIX constant maturity instruments. By working closely with Avellaneda (2021) work,we first use variables extracted from the VIX term structure to construct various VIX term-structure-related features and combine major financial market features, we then implement various machine learning methods to predict the next day return of six VIX CMFs. To asses the economic impact among models, we utilise machine learning prediction results as trading signals, implementing two trading strategies: The long-short strategy and constrained mean-variance optimization(C-MVO) strategy.
\\The key contributions of this study can be summarised as follows,  firstly, leveraging the VIX CMFs decomposition methods proposed by Avellaneda (2021), we demonstrate that the term structure information \(\mu_t\) and \(\triangle roll\) extracted from \(V_t\) and \(roll_t\) is highly informative for predicting next-day returns of VIX CMFs, and could bring economic benefits based on our back-testing analysis. However, we also found that time-series derivatives contribute minimally to predictive enhancements. \textit{Second, various state-of-art machine learning approaches are introduced to predict the VIX rolling-ETFs next-day returns.} Thirdly, our proposed Constrained-Mean-Variance Portfolio Optimization (C-MVO) strategy exhibits superior back-testing performance across most models compared to the long-short strategy, providing valuable insights for researchers and practitioners looking to develop VIX CMFs trading strategies by following the path of numerical predictions. The remainder of the paper is organised as follows: Section 2 details the decomposition methods of CMFs term structure and the machine learning approaches employed.  Section 3 describes data preparation and experiment designs, Section 4 analyse and discuss results, Appendix includes supporting graphs and tables.



\section*{Research Methodologies and Data}
\subsection*{1. Trading signals constructions}
The VIX is calculated by the Chicago Board Options Exchange (CBOE) and is based on the prices of options on the S\&P 500 Index.
\\Let $t$ denote time and let $VIX_t$ denote the value of VIX on that date. At time $t$,
we can view lots of VIX futures contracts with different expiration dates:
\begin{eqnarray}
    \mathrm{F^i_t} := \text{VIX futures expiring at time $T_i$ at time t}
\end{eqnarray}
where $i$ denote the index of the VIX futures contracts with a ascending expiration dates $T_1 < T_2 < ... < T_d$.
\\A term-structure of constant-maturity VIX futures(CMFs), consist of \({1,2,...,6}\) months in our case, are constructed as
a linear interpolation of two nearest expiration dates VIX futures:
\begin{eqnarray}
\label{eq:defineV}
    \mathrm{V^j_t} := \omega^j_tF^{L_j}_t\ + (1 - \omega^j_t)F^{R_j}_t \text{for j in \{1,2,...,6\}}
\end{eqnarray}
where $L_j$ represents the VIX futures of the nearest expiration dates close to $j$-month but with sooner expiration period than $j$-month,
$R_j$ represents the VIX futures of the nearest expiration dates close to $j$-month but with later expiration period than $j$-month, and
$\omega^j_t = \frac{T_{R_j} - 30j}{T_{R_j} - T_{L_j}}$. Note that $VIX_t$ is like a zero-horizon CMF.
CMFs are commonly used for they do not suffer fluctuations caused by contract expiry.

\subsubsection*{Rolling VIX Futures Derivatives and Strategies}
By using rolling VIX futures derivatives or strategies, we can replicate the daily returns of CMFs.
SPVXSP, SPVIX2ME, SPVIX3ME, SPVIX4ME, SPVXMP, SPVIX6ME represents 1-6 months CMFs, they can either be traded with published financial derivatives or replicated with rolling VIX futures strategies.
Derivatives like ETFs and ETNs or Strategies maintains the CMF weights of equation ~\ref{eq:defineV} for fixed maturity $j-month$. For each $j$, we denote $R^j$
as the daily return of rolling VIX futures strategy, trading cost is not considered:
\begin{eqnarray}
    \mathrm{R^j_t} = \frac{\omega^j_t{\Delta}F^{L_j}_t\ + (1 - \omega^j_t){\Delta}F^{R_j}_t}{\omega^j_tF^{L_j}_t\ + (1 - \omega^j_t)F^{R_j}_t}
\end{eqnarray}
where${\Delta}F^{L_j}_t = F^{L_j}_{t+1} - F^{L_j}_t$. We denote ${\Delta}t = \frac{1}{252}$, transform equation in terms of the CMFs:
\begin{eqnarray}
\label{eq: $R^j_t$2}
    \mathrm{R^j_t} = \frac{{\Delta}V^j_t}{V^j_t} - \frac{F^{R_j}_{t+1} - F^{L_j}_{t+1}}{V^j_t(T_{R_j} - T_{L_j}){\Delta}t}{\Delta}t
\end{eqnarray}
Let Roll denote the drift term in equation \ref{eq: $R^j_t$2}, it referred to as the roll yield of rolling VIX futures strategy:
\begin{eqnarray}
\label{roll}
    \mathrm{Roll^j_{t+1}} := - \frac{F^{R_j}_{t+1} - F^{L_j}_{t+1}}{V^j_t(T_{R_j} - T_{L_j}){\Delta}t}
\end{eqnarray}
In a Contango circumstances, $F^{R_j}_{t+1} > F^{L_j}_{t+1}$ means $Roll < 0$. On the opposite, in a Backwardation circumstances $Roll > 0$.
We re-write equation as follows:
\begin{eqnarray}
\label{eq: $R^j_t$3}
    \mathrm{R^j_t} = \frac{{\Delta}V^j_t}{V^j_t} + Roll^j_{t}{\Delta}t
\end{eqnarray}
From equation \ref{eq: $R^j_t$3} we see $R^j_t$ consist of two parts, the change in $V^j_t$ and roll yield. As history data shown, the most likely VIX futures
curves are contango, so the value of the rolling VIX futures strategies decay.
\\Avellaneda(2019) shows that volatility is a mean-reversion process, in an ideal world, suppose $V^j_t$ and CMFs are both stable for long-period such as one month, then
the return of $V^j_t$ should be $V^j_t - V^{j-1}_t$, so we denote $\mu$ as :
\begin{eqnarray}
\label{eq: mu}
    \mathrm{\mu^j_t} = V^j_t - V^{j-1}_t
\end{eqnarray}
$\mu^j_t$ contains instantaneous market views of return about $V^j_t$.

\subsection*{2. Proposed Machine Learning Algorithms}

\subsubsection*{2.1 XGBoost}
 The concept of boosting, proposed by Freund and Schapire (1999), revolves around an ensemble model strategy that combines multiple weak learners to create a more robust learner. Friedman (1999) introduced Gradient Tree Boosting, an improvement in boosting that incorporates the gradient descent algorithm into the loss function to minimize errors. Expanding on gradient boosting, Chen and Guestrin (2016) introduce XGBoost, a scalable end-to-end tree boosting system based on additive training strategy that augments gradient tree boosting with regularization, efficient handling of sparse data, and enhanced computational efficiency, resulting in superior speed and accuracy. Eq.\ref{eq:schemeP} denotes the objective function of XGBoost where \(\mathbf{\sum_{i=1}^{n}l(y_i,\hat{y}_i)}\) represents the training loss function and \(\mathbf{{\sum_{k=1}^{K}}\Omega(f_k)}\) denotes the complexity of trees.
\begin{eqnarray}
\label{eq:schemeP}
\mathrm{Obj}=\underbrace{\mathbf{\sum_{i=1}^{n}l(y_i,\hat{y}_i)}}_{Training \: Loss} +\underbrace{\mathbf{{\sum_{k=1}^{K}}\Omega(f_k)}}_{Complexity \: of \: trees}
\end{eqnarray}


\subsubsection*{2.2 Multilayer Perceptron (MLP)}
The Multi-Layer Perceptron (MLP) is a class of feed-forward artificial neural network (ANN). It consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Unlike a linear perceptron, each node (or neuron) in the MLP uses a nonlinear activation function. This aspect of MLP allows it to distinguish data that is not linearly separable, which is a limitation of the basic perceptron. MLPs are widely used for solving complex problems in machine learning, including pattern recognition and classification tasks, by learning a non-linear function approximator for either classification or regression. They are trained using back propagation, ensuring that the error between the actual and predicted outputs is minimized. Due to their ability to capture complex relationships in data, MLPs have become a fundamental tool in the field of deep learning and neural networks.
In our experiment, we use leaky ReLU introduced by Mass (2013) as the activation function \(\phi\) for MLP, the mathematical expression of leaky ReLU  is shown in Eq.2. Given activation function \(\phi\), features \(X \in R^ {n\times d}\) where \(x\) presents the batch size and \(d\) presents the dimensionalities
\begin{eqnarray}
\label{eq:schemeP}
\mathrm{\phi^{(i)}} =\mathbf{max({W^{(i)}}^{T}x,0)}=
\begin{cases}
\mathbf{{W^{(i)}}^{T}x},\quad &\mathbf{{W^{(i)}}^{T}x}>0\\0, \quad else
\end{cases}
\end{eqnarray}
where \(W^{(i)}\) is the weight vector of \(i_{th}\) hidden unit while \(x\) is the input features. A naive MLP can be illustrated as fig.\ref{MLP}
\begin{figure}[!h]
%\includegraphics[width=0.5\linewidth]{MLP.jpg}
\caption{{\bf Naive MLP}}
\label{MLP}
\end{figure}

\subsubsection*{2.3 Recurrent Neural Network (RNN)}
We then implement multiple machine learning models based on RNN architecture which aims to find patterns in sequences of data. The work of Schmidt (2019) provides a comprehensive overview of the basic architecture and functions of RNNs while Sherstinsky (2023) work focuses on deriving the classical RNN formulation from differential equations and addresses the challenges in training standard RNNs. The transformation of RNN into the "Vanilla LSTM" network is explained through logical arguments, with a detailed description of the LSTM system's equations and entities.
RNN train a function $f$:
\begin{eqnarray}
\label{eq: RNN}
    {h_{t}, y_t} = f(h_{t - 1}, x_t)
\end{eqnarray}
where $h$ represents hidden layer, it carry sequence information by $t - 1$, $x_t$ is the input at $t$, $f$ outputs result $y_t$ at $t$ and sequence information $h_t$.

\subsubsection*{2.3.1 Long Short-term Memory (LSTM)}
The Long Short-Term Memory (LSTM) model, originally proposed by Hochreiter and Schmidhuber (1997), represents a significant milestone in the field of neural networks, particularly in handling sequential data. LSTMs were designed to overcome the limitations of traditional RNNs, especially issues related to learning long-range dependencies. Traditional RNNs struggled with the vanishing and exploding gradient problems, making it challenging to retain information over long sequences. The unique memory cells and input, forget and output gates mechanism enables LSTM to store important information and forget irrelevant details over long sequences. With the recent innovation of attention mechanism, Wang and Hao (2020) introduced the Attention-based LSTM (ALSTM) model, they integrates a multi-head dot product attention within the LSTM architecture and significantly enhances model ability for complex reasoning over sequences.



Eq \ref {eq: LSTM} and Eq \ref {eq: ALSTM1},\ref {eq: ALSTM2} illustrates the mathematical details for the implementation of LSTM and ALSTM in our study.
LSTM framework denote $c$ as the long-term sequence information distinguish from $h$, and design gates to filter information. A gate cell can be represent as:
\begin{eqnarray}
\label{eq: Gate}
    g = \sigma(W\bullet[h_{t - 1}, x_t] + b)
\end{eqnarray}
where $W, b$ are trainable parameters, $\sigma$ represent activation function, so $g$ can be use as a gate. LSTM framework design three gate on RNN as follow:
\begin{eqnarray}
\label{eq: LSTM}
    \begin {cases} c_t = g^f{\odot}c_{t - 1} - g^i{\odot}tanh(W\bullet[h_{t - 1}, x_t] + b)
    \\h^t = g^o{\odot}tanh(c_t)
    \\y^t = f(h_t)
    \end {cases}
\end{eqnarray}
$g^f, g^i, g^o$ represent three gates, and $f$ is some function link LSTM outputs to downstream task.
\\ALSTM add a encoder and a decoder attention-based layers in LSTM, Given $n$ series, stage one encoder layer can learn attention at a same time and transform to new input.
\begin{eqnarray}
\label{eq: ALSTM1}
    \begin {cases} e^k_t = tanh(W\bullet[h_{t - 1}, c_{t - 1}] + bx^k)
    \\{\alpha}^k_t = \frac{exp(e^k_t)}{\sum_{i=1}^n exp(e^i_t)}
    \\\widetilde{x_t} = ({\alpha^1_t}x^1_t, {\alpha^2_t}x^2_t,..., {\alpha^n_t}x^n_t)
    \end {cases}
\end{eqnarray}
with $\widetilde{x_t}$ as input, stage one construct with a LSTM framework named $L_1$, stage two decoder layer learn attention with all the hidden state $h_t$ of $L_1$,
the output $\widetilde{h_t}$ transform the label $y$ of another LSTM framework named $L_2$, so two attention layers can learn relations between $n$ series.
Let $d$ denote the hidden state of $L_2$, transform of $y$ can be formulate as:
\begin{eqnarray}
\label{eq: ALSTM2}
    \begin {cases} l_t = tanh(W\bullet[d_{t - 1}, c^2_{t - 1}] + bh^i)
    \\{\beta}_t = \frac{exp(l_t)}{\sum_{i=1}^T exp({\beta}_t)}
    \\c_t = \sum_{i=1}^T {\beta}_ih_i
    \\\widetilde{y_{t - 1}} = W\bullet[y_{t - 1}; c_{t - 1}] + b
    \end {cases}
\end{eqnarray}
$\tilde{y}$ is the input of $L_2$.

\subsubsection*{2.3.2 Gated Recurrent Unit}
 The Gated Recurrent Unit (GRU), first introduced by Cho et al. (2014), is designed to capture temporal dependencies in sequential data efficiently. This efficiency in GRU is achieved through its unique architecture, consisting of two gates: the update gate and the reset gate. The update gate plays a crucial role in determining how much of its previous state the GRU retains, enabling the effective capture of long-term dependencies. Conversely, the reset gate influences the amount of past information to be forgotten, assisting the model in concentrating on the most pertinent information. These gates effectively control the flow of information within the unit, allowing the network to retain important information from past data inputs while discarding irrelevant data.This selective memory mechanism empowers the GRU to effectively tackle the vanishing gradient problem, a common challenge in traditional RNNs.The implementation details can be found in eq.4.

\begin{eqnarray}
\label{eq: GRU}
    \begin {cases} h^{'}_{t - 1} = h_{t - 1}{\bullet}g^r
    \\h^{'} = tanh(W\bullet[h^{'}_{t - 1}, x_t] + b)
    \\h^t = (1 - g^z){\odot}tanh(c_t) + g^z{\odot}h^{'}
    \end {cases}
\end{eqnarray}
Follow notation in LSTM,  $g$ represents gate \ref{eq: Gate}.

%\subsubsection*{2.4 Temporal Covolutional Networks}
% Temporal covolutional networks (TCN), originally introduced by Lea et al. (2016), is a type of neural network that employs convolutional layers to capture dependencies in sequential data. Unlike traditional RNN and LSTMs, TCNs use 1D convolutions to process input sequences in parallel, making them highly parallelizable and capable of capturing long-range dependencies. Implementation details can be found in eq.5.

% \begin{eqnarray}
% \label{eq: TCN}
%     TCN = FCN + \text{causal convolutions}
% \end{eqnarray}
% where FCN is fully-convolutional network with one dimension.


% \subsubsection* {2.5 Transformer}
% We also implement attention-based network architecture called transformer proposed by Vaswani et al. (2017) for prediction. Transformer departs from the traditional recurrent or convolutional layers, relying instead on the attention mechanism, This enables the model to weigh the significance of different parts of the input data differently, allowing it to capture complex dependencies and relationships within the data. Eq. illustrates the implementation details.

% \begin{eqnarray}
% \label{eq: Transformer}
%     y_i = F(encoder(X), decoder(y_1,...,y_{i - 1}))
% \end{eqnarray}
% In our case, we only use encoder to produce output, for we don't need to produce output one by one. Encoder's framework follows by:
% \begin{eqnarray}
% \label{eq: TransformerDecoder}
%     \begin {cases} \tilde{X} = \text{Positional Encoding} + X
%     \\Y = FFN(\text{self-attention}(X))
%     \end {cases}
% \end{eqnarray}
% where FFN is a feedforward network with residual added and layer norm, self-attention's framework follows by:
% \begin{eqnarray}
% \label{eq: self-attention}
%     \begin {cases} Q = W^QX
%     \\K = W^KX
%     \\V = W^VX
%     \\Z = \test{softmax}(QK^T)V
%     \end {cases}
% \end{eqnarray}
% where $Z$ is the output of self-attention layer.
\subsection*{Data Preparations and Experiment Designs}
\subsubsection*{Data Preparations}
In line with our primary goal of predicting next-day arithmetical return of CMFs. it is rational to initially incorporate CMFs-related factors \(V^j_t\) and \(Roll^j_{t}\) as detailed in Eq.\ref{eq:defineV} and Eq.\ref{roll} Additionally, to thoroughly examine the informative power of term structures, we introduce \(\mu^j_t\) denoted in Eq.\ref{eq: mu} and  \(\triangle Roll_{t}^j\)to provides a comprehensive description of the CMFs term structure. We also integrate \(TLT\) and \(SPY\) into our analysis to partially represents the macro-economics conditions. Furthermore, various time-series statistical transformations are employed to explore possible extra information with details provided in table \ref{table_all}. The time range for \(VIX\)  futures and \(CMFs\)  data covers the period from January 3, 2005, to March 7, 2023 while \(TLT\)  and \(SPY\)  data are selected within the time frame of January 12, 1995, to April 12, 2023. Three distinct data sets are utilised to incrementally examine the informative power of VIX term structures and additional information offered by time-series statistical transformations.
Detailed descriptions of all three datasets are provided in tables \ref{table_simple}, \ref{table_term} and \ref{table_all}.
\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Simple Data sets}}
\begin{tabular}{|c|c|c|} % Use c for centered columns; you can also use other alignment options like l (left) or r (right)
    \hline % Horizontal line at the top
    \(VIX_{CMFs}\)  & CMFs-related factors & Macro-factors \\
    \hline % Horizontal line under column headers
    \(CMF_{1m}\)  & \({[V_{t}^1},{Roll_{t}^1]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) \\
    \hline
    \(CMF_{2m}\)  & \({[V_{t}^2},{Roll_{t}^2]}\)  & \([log_{SPY_t},log_{TLT_t},VIX_t]\) \\
    \hline
    \(CMF_{3m}\)  & \({[V_{t}^3},{Roll_{t}^3]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) \\
    \hline
    \(CMF_{4m}\)& \({[V_{t}^4},{Roll_{t}^4]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\)\\
    \hline
    \(CMF_{5m}\) &  \({[V_{t}^5},{Roll_{t}^5]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) \\
    \hline
    \(CMF_{6m}\) & \({[V_{t}^6},{Roll_{t}^6]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) \\
    \hline % Horizontal line at the bottom
  \end{tabular}
\begin{flushleft} Table notes: The initial data sets comprise solely of CMFs-related factors and macroeconomic variables.
\end{flushleft}
\label{table_simple}
%\end{adjustwidth}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Term Structure Data sets}}
\begin{tabular}{|c|c|c|c|c|c|} % Use c for centered columns; you can also use other alignment options like l (left) or r (right)
    \hline % Horizontal line at the top
    \(VIX_{CMFs}\)  & CMFs-related factors & Macro-factors & Term Structures factors \\
    \hline % Horizontal line under column headers
    \(CMF_{1m}\)  & \({[V_{t}^1},{Roll_{t}^1]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^1, \triangle Roll_{t}^1]}\)  \\
    \hline
    \(CMF_{2m}\)  & \({[V_{t}^2},{Roll_{t}^2]}\)  & \([log_{SPY_t},log_{TLT_t},VIX_t]\) &  \({[\mu_{t}^2, \triangle Roll_{t}^2]}\) \\
    \hline
    \(CMF_{3m}\)  & \({[V_{t}^3},{Roll_{t}^3]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^3, \triangle Roll_{t}^3]}\) \\
    \hline
    \(CMF_{4m}\)& \({[V_{t}^4},{Roll_{t}^4]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^4, \triangle Roll_{t}^4]}\)\\
    \hline
    \(CMF_{5m}\) &  \({[V_{t}^5},{Roll_{t}^5]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^5, \triangle Roll_{t}^5]}\) \\
    \hline
    \(CMF_{6m}\) & \({[V_{t}^6},{Roll_{t}^6]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^6, \triangle Roll_{t}^6]}\)  \\
    \hline % Horizontal line at the bottom
  \end{tabular}
\begin{flushleft} Table notes: Term structures have been added to the data sets based on the simple datasets.
\end{flushleft}
\label{table_term}
%\end{adjustwidth}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{sidewaystable}
\begin{table}[!ht]
\caption{{\bf Statistical Derivations Data Sets}}
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|}% Use c for centered columns; you can also use other alignment options like l (left) or r (right)
    \hline % Horizontal line at the top
    \(VIX_{CMFs}\)  & CMFs-related Factors & Macro-factors & Term Structure Factors & Time-series Statistical Derivations\\
    \hline % Horizontal line under column headers
    \(CMF_{1}\)  & \({[V_{t}^1},{Roll_{t}^1]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^1, \triangle Roll_{t}^1]}\) &
    \makecell[c]{\([Std_{T}(All_{t}^{1}), Skew_{T}^{1}(All_{t}^{1}),Kurt_T(All_{t}^{1}),Mean_{T}(All_{t}^{1})\),\\ \(z\ score_{T}(term_{t}^{1})\)\\ , \(where \: T \in \{5,20, 60\}\)]}   \\
    \hline
    \(CMF_{2}\)  & \({[V_{t}^2},{Roll_{t}^2]}\)  & \([log_{SPY_t},log_{TLT_t},VIX_t]\) &  \({[\mu_{t}^2, \triangle Roll_{t}^2]}\) &   \makecell[c]{\([Std_{T}(All_{t}^{2}), Skew_{T}^{2}(All_{t}^{2}),Kurt_T(All_{t}^{2}),Mean_{T}(All_{t}^{2})\),\\ \(z\ score_{T}(term_{t}^{2})\)\\ , \(where \: T \in \{5,20, 60\}\)]} \\
    \hline
    \(CMF_{3}\)  & \({[V_{t}^3},{Roll_{t}^3]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^3, \triangle Roll_{t}^3]}\) &  \makecell[c]{\([Std_{T}(All_{t}^{3}), Skew_{T}^{3}(All_{t}^{3}),Kurt_T(All_{t}^{3}),Mean_{T}(All_{t}^{3})\),\\ \(z\ score_{T}(term_{t}^{3})\)\\ , \(where \: T \in \{5,20, 60\}\)]}  \\
    \hline
    \(CMF_{4}\)& \({[V_{t}^4},{Roll_{t}^4]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^4, \triangle Roll_{t}^4]}\) &   \makecell[c]{\([Std_{T}(All_{t}^{4}), Skew_{T}^{4}(All_{t}^{4}),Kurt_T(All_{t}^{4}),Mean_{T}(All_{t}^{4})\),\\ \(z\ score_{T}(term_{t}^{4})\)\\ , \(where \: T \in \{5,20, 60\}\)]} \\
    \hline
    \(CMF_{5}\) &  \({[V_{t}^5},{Roll_{t}^5]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^5, \triangle Roll_{t}^5]}\) &  \makecell[c]{\([Std_{T}(All_{t}^{5}), Skew_{T}^{5}(All_{t}^{5}),Kurt_T(All_{t}^{5}),Mean_{T}(All_{t}^{5})\),\\ \(z\ score_{T}(term_{t}^{5})\)\\ , \(where \: T \in \{5,20, 60\}\)]} \\
    \hline
    \(CMF_{6}\) & \({[V_{t}^6},{Roll_{t}^6]}\) & \([log_{SPY_t},log_{TLT_t},VIX_t]\) & \({[\mu_{t}^6, \triangle Roll_{t}^6]}\) & \makecell[c]{\([Std_{T}(All_{t}^{6}), Skew_{T}^{6}(All_{t}^{6}),Kurt_T(All_{t}^{6}),Mean_{T}(All_{t}^{6})\),\\ \(z\ score_{T}(term_{t}^{6})\)\\ , \(where \: T \in \{5,20, 60\}\)]}  \\
    \hline % Horizontal line at the bottom
\end{tabular}}
\begin{flushleft} Table notes: \( All_{t}^{j} \in \{CMFs\ related \ Factors, Macro \ factors, Term\ Structure\ Factors\}\) and \(term_{t}^{j} \in \{Term\ Structure\ Factors \}\)
\end{flushleft}
\label{table_all}
\end{adjustwidth}
\end{table}
%\end{sidewaystable}

 \subsubsection*{Empirical Experiment Designs}
 The main goal of our empirical experiment aims to evaluate the statistical and backtesting performances among different models and different datasets. Before training, we are dealing with 6 groups of instruments and features under a single date as shown in table \ref{table_simple}, \ref{table_term} and \ref{table_all}. Based on the concept of "feature stacking", we combine 6 groups of instruments and features into one large group to achieve a more general conclusions. Meanwhile, the model is less biased by outliers and over-fitting is less likely to occur while correlations among prediction targets are not considered in the model. During training, cross-validation is widely used to determine the generalization error of an ML algorithm, so as to prevent over fitting. However, CV can be exposed to future information leakage, particularly in time-series data due to various reasons such as the inclusion of future data points caused by random sampling, the temporal dependencies of financial time-series data, etc. Therefore, to avoid the leakage of future information, we implement a walk-forward training and back-testing procedure, specifically, We set (TRAIN, VALID, TEST) as follows: TRAIN: (2005-12-20, 2010-06-30), VALID: (2010-07-01, 2010-12-31), TEST: [2011-01-01, 2022-08-15]. This setup is based on three main considerations:
 \\1. To ensure a sufficiently long back testing period of 11 years; \\2. To ensure the first training set is large enough and includes extreme market conditions like the 2008 financial crisis;
 \\3. To set a six-month valid set, ensuring the training data used for predictions is at least six months old, which further prevents information leakage and increases robustness. We believe that using more recent data in practice can yield better results;
 \\4. we follow an expanding window process  when using the data, meaning each training includes all previous data points, to train a more robust model for prediction;

 \begin{algorithm}
        \caption{walking forward procedure}
        \begin{algorithmic}
            \STATE $rollingtasks \gets Algorithm TaskRollGen\ref{Task_Roll_Gen}$
            \FOR{$task \gets rollingtasks$}
            \STATE $model \xleftarrow{fit} task[train]$
            \STATE $pred \xleftarrow{predict} task[test]$
            \STATE $preds \xleftarrow{append} pred$
            \ENDFOR
            \STATE $PRED \xleftarrow{concat} preds \text{\quad \# PRED is a time series cover Task[test]'s horizon}$
            \STATE $metrics \xleftarrow{calculate} PRED$
            \STATE $backtestingresult \gets PRED, strategy \text{\quad \# bar by bar}$
        \end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Task Roll Gen}
	\label{Task_Roll_Gen}
	\begin{algorithmic}
    	\REQUIRE TASK contains \{TRAIN, VALID, TEST, roll\_lenth\}
            \ENSURE rollingtasks with their own \{train, valid, test\}
            \STATE $fragments \gets$ Test divided by roll\_lenth
            \FOR{$m \gets fragments$}
            \STATE $task \gets Task$
            \STATE $task[test] \gets m$
            \STATE $task[valid] \gets [start\_date_{test} - 1 - lenth_{Valid}, start\_date_{test} - 1]$
            \STATE $task[train] \gets [start\_date_{TRAIN}, strat\_date_{valid} - 1]$ \text{\quad \# expanding}
            \STATE $rollingtasks \xleftarrow{append} task$
            \ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsubsection*{Back-testing Performance Evaluation Methods:  Constrained-mean-variance Optimization (C-MVO) strategy \& Long-short strategy}

We introduce both C-MVO strategy and long-short strategy to explore the backtesting performances among various machine learning models. The following is the detailed explanation for two strategies.  In line with the classical mean-variance portfolio optimization structure proposed by Markowits (1952).  For C-MVO strategy, we incorporated several additional constraints. The \(Cov_{port}\) was derived through the estimation of a 60-days historical covariance matrix. The risk aversion parameter, denoted as \(gamma\), was fixed at a value of 0.2. To ensure prudent risk management, we imposed constraints based on portfolio characteristics. Specifically, we restricted gross leverage to a maximum of 3, and imposed absolute limits on the individual weights of cmfs, ensuring individual weights did not exceed an absolute value of 1. Furthermore, a critical risk threshold was established, with the annualized risk of the portfolio constrained to not exceed 30 \%.
This strategy provides a rigorous approach to portfolio construction, striking a balance between expected returns and risk mitigation. The application of historical covariance data and the delineation of precise risk parameters contribute to the robustness and reliability of the derived portfolio weights, thereby enhancing the integrity of the investment strategy. The detailed optimization function is represented in eq.\ref{MVO}
 For long-short strategy, which is a common trading strategy aims to exploit profits from relative performances of assets through a combination of long and short positions. In our strategy design,we rank six \(Cmfs\) instrument based on predicted returns in descending order. Each trading day, we maintain an equal long and short positions between CMFs with largest predicted return and lowest predicted return, with daily re-balance.
\begin{eqnarray}
\label{MVO}
    Max_{w_i}: \quad & \sum_{i=1}^{n} (W_i \times R_i)-gamma \times risk \nonumber \\
\text{s.t.:} \quad & max(|w_i|) \leq 1 \nonumber \\
                         & \sum_{i=1}^{N} |w_i| \leq 3 \\
        & \left| \sum_{i=1}^{N} w_i \right| \leq 2 \nonumber \\
        & risk \leq risk_{max} \nonumber \\
        & where \quad risk=W_i \times Cov_{port} \times W_{i}^T, \nonumber \\
        & risk_{max}=0.03, \quad gamma=0.2 \nonumber
\end{eqnarray}

% For figure citations, please use "Fig" instead of "Figure".




% Results and Discussion can be combined.
\section*{Results}
Panel.\ref{summary} presents the combined results of predictive and back-testing performances, to assess prediction performance across all machine learning models, we have employed a range of metrics within the domain of quantitative trading evaluation systems. Our major metric is the information coefficient (IC), a statistical measure that quantifies the correlations between predictions of returns and subsequent realized returns of portfolio which can be denoted as \(Corr(\hat{y},{y})\)  A high positive IC indicates a robust predictions are highly correlated with actual returns, suggesting a strong predictive ability. Conversely, a low or negative IC suggests inaccuracies in prediction or even contrarian to actual outcomes. Furthermore, we have integrated IR (Information  Ratio) into our evaluation, which can be calculated as Eq.\ref{IR}. In addition, we have introduced ICIR, Rank IC that replace \(Corr(\hat{y},{y})\) as the correlation of returns rankings denoted as \(Corr(\hat{rank},{rank})\) and subsequently computed Rank ICIR. We consider information ratio (see explanations in Goodwin (1998) work) and annualised returns as key indicators for the assessments of back-testing performances.

\begin{eqnarray}
\label{IR}
IR \approx \frac{\bar{IC}}{Std(IC)}
\end{eqnarray}


\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{all result}
% \begin{sidewaystable}
\begin{tabular}{llrrrrp{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
    &               &     IC &   ICIR &  Rank IC &  Rank ICIR &  C-MVO Annualized Return &  C-MVO Information Ratio &  Long Short Annualized Return &  Long Short Information Ratio \\
model & dataset &        &        &          &            &                          &                          &                               &                               \\
\midrule
ALSTM & Derivations &  0.040 &  0.066 &    0.027 &      0.048 &                   -0.002 &                   -0.016 &                         0.033 &                         0.174 \\
    & Simple &  0.005 &  0.007 &   -0.005 &     -0.008 &                    0.057 &                    0.803 &                         0.051 &                         0.258 \\
    & TermStructure &  0.029 &  0.042 &    0.026 &      0.041 &                    0.071 &                    0.744 &                         0.112 &                         0.579 \\
\midrule
GRU & Derivations &  0.021 &  0.035 &    0.017 &      0.031 &                    0.049 &                    0.310 &                         0.096 &                         0.641 \\
    & Simple & -0.023 & -0.033 &   -0.026 &     -0.040 &                    0.045 &                    0.517 &                         0.059 &                         0.310 \\
    & TermStructure &  0.004 &  0.006 &    0.002 &      0.004 &                    0.028 &                    0.241 &                         0.080 &                         0.409 \\
\midrule
LGB & Derivations &  0.019 &  0.059 &    0.085 &      0.127 &                    0.020 &                    0.264 &                        -0.085 &                        -0.355 \\
    & Simple & -0.024 & -0.050 &   -0.044 &     -0.069 &                   -0.004 &                   -0.044 &                        -0.121 &                        -0.511 \\
    & TermStructure &  0.035 &  0.066 &    0.054 &      0.079 &                    0.023 &                    0.327 &                        -0.085 &                        -0.367 \\
\midrule
LSTM & Derivations &  0.040 &  0.067 &    0.036 &      0.064 &                    0.047 &                    0.416 &                         0.020 &                         0.123 \\
    & Simple & -0.015 & -0.022 &   -0.026 &     -0.042 &                    0.030 &                    0.388 &                         0.044 &                         0.262 \\
    & TermStructure &  0.021 &  0.031 &    0.016 &      0.026 &                    0.059 &                    0.671 &                         0.034 &                         0.168 \\
\midrule
Linear & Derivations &  0.095 &  0.147 &    0.079 &      0.140 &                    0.165 &                    1.049 &                         0.117 &                         0.646 \\
    & Simple &  0.090 &  0.147 &    0.070 &      0.131 &                    0.068 &                    1.325 &                         0.186 &                         0.992 \\
    & TermStructure &  0.114 &  0.162 &    0.091 &      0.152 &                    0.150 &                    2.291 &                         0.214 &                         1.105 \\
\midrule
MLP & Derivations & -0.011 & -0.018 &   -0.013 &     -0.024 &                   -0.023 &                   -0.074 &                         0.056 &                         0.325 \\
    & Simple & -0.002 & -0.004 &   -0.001 &     -0.001 &                    0.195 &                    0.630 &                        -0.009 &                        -0.046 \\
    & TermStructure &  0.011 &  0.017 &    0.012 &      0.020 &                    0.013 &                    0.043 &                         0.050 &                         0.302 \\
\midrule
XGB & Derivations &  0.008 &  0.012 &    0.011 &      0.019 &                    0.062 &                    0.250 &                         0.024 &                         0.141 \\
    & Simple &  0.015 &  0.023 &    0.003 &      0.005 &                   -0.019 &                   -0.073 &                         0.042 &                         0.260 \\
    & TermStructure &  0.047 &  0.071 &    0.043 &      0.069 &                    0.011 &                    0.044 &                         0.116 &                         0.632 \\
\bottomrule
\end{tabular}
\begin{flushleft} Table notes:(WTA)
\end{flushleft}
\label{summary}
% \end{sidewaystable}
\end{adjustwidth}
\end{table}

\subsection*{Factors Evaluations}
Initially, from the perspective of three datasets, based on panel.\ref{summary}, for the predictive performances, models with simple datasets have shown the worst IC compared to the rest of two datasets containing term structure factors and time-series derivations. Notably, GRU (-0.023), LGB(-0.024), LSTM(-0.015) and MLP(-0.002) even shows negative IC which indicates the predicted returns are contrast with realised returns. By following this perspective, we discover  first findings that the robust informative power of term structure factors exists which will be explained in details in the following subsections. In our experiment, we explore three distinct datasets named Simple, TermStructure, Derivations as before, our results show that TermStructure and Derivations with term structure infomation stand out.
\\Table~\ref{Best Dataset} select the best datasets for all models based on main metrics: IC, rank IC, information ratio of long short strategy, information ratio of constrained mean-variance optimization strategy.
Data shows Simple dataset doesn't prevail in any model on prediction metrics and backtesting metrics. 46 \% for TermStructure and 39\% for derivations.As described before, TermStructure mainly extent term structure features like $\mu$ and $\Delta roll$, Derivations extent time series distribution features like rolling-ma and z-score on TermStructure features. Term structure features do improve the result, but time series distribution features don't improve obviously.We will only discuss TermStructure below.

\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Best Dataset}
\label{Best Dataset}
% \begin{sidewaystable}
\begin{tabular}{lllll}
\toprule
{} &             IC &        Rank IC & C-MVO Information Ratio & Long Short Information Ratio \\
model  &                &                &                         &                              \\
\midrule
ALSTM  &    Derivations &    Derivations &                  Simple &                TermStructure \\
GRU    &    Derivations &    Derivations &                  Simple &                  Derivations \\
LGB    &  TermStructure &    Derivations &           TermStructure &                  Derivations \\
LSTM   &    Derivations &    Derivations &           TermStructure &                       Simple \\
Linear &  TermStructure &  TermStructure &           TermStructure &                TermStructure \\
MLP    &  TermStructure &  TermStructure &                  Simple &                  Derivations \\
XGB    &  TermStructure &  TermStructure &             Derivations &                TermStructure \\
\bottomrule
\end{tabular}
\begin{flushleft} The best dataset in all models on main metircs: IC, Rank IC, C-MVO Information Ratio, Long Short Information Ratio.
\end{flushleft}
\label{table1}
% \end{sidewaystable}
\end{adjustwidth}
\end{table}
Throughout the 11-year span from 2011 to 2022, the evaluation of TermStructure of 7 machine learning models was conducted using the Walk-Forward Training approach. This method, known for its strick out-of-sample in time series prediction and strategy, also involves iterative training, enabling models to adapt dynamically to evolving data characteristics.
table \ref{Statistic Metric} shows the the average Information Coefficient (IC) value of 0.037 which prove the existence of term structure factors informative power and its ability to provide valuable insights.Across all models' backtesting, an average Information Ratio (IR) of 0.623 was achieved as illustrated in  Table~\ref{Backtest Metric}. This notable average Information Ratio of 0.623 underscores the dataset's effectiveness in not only facilitating accurate predictions, as previously discussed, but also in translating these predictions into actionable and profitable trading strategies like long short strategy and constrained mean-variance strategy.

\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Predictive Metrics}
\label{Statistic Metric}
% \begin{sidewaystable}
\begin{tabular}{lrrrr}
\toprule
{} &     IC &   ICIR &  Rank IC &  Rank ICIR \\
model  &        &        &          &            \\
\midrule
ALSTM  &  0.029 &  0.042 &    0.026 &      0.041 \\
GRU    &  0.004 &  0.006 &    0.002 &      0.004 \\
LGB    &  0.035 &  0.066 &    0.054 &      0.079 \\
LSTM   &  0.021 &  0.031 &    0.016 &      0.026 \\
Linear &  0.114 &  0.162 &    0.091 &      0.152 \\
MLP    &  0.011 &  0.017 &    0.012 &      0.020 \\
XGB    &  0.047 &  0.071 &    0.043 &      0.069 \\
mean   &  0.037 &  0.056 &    0.035 &      0.056 \\
\bottomrule
\end{tabular}
\begin{flushleft}
Table notes: performances based on statistic metrics
\end{flushleft}
\label{table1}
% \end{sidewaystable}
%\end{adjustwidth}
\end{table}



\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Backtesting Metrics}
\label{Backtest Metric}
% \begin{sidewaystable}
\begin{tabular}{lp{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
{} &  C-MVO Annualized Return &  C-MVO Information Ratio &  Long Short Annualized Return &  Long Short Information Ratio \\
model  &                          &                          &                               &                               \\
\midrule
ALSTM  &                    0.071 &                    0.744 &                         0.112 &                         0.579 \\
GRU    &                    0.028 &                    0.241 &                         0.080 &                         0.409 \\
LGB    &                    0.023 &                    0.327 &                        -0.085 &                        -0.367 \\
LSTM   &                    0.059 &                    0.671 &                         0.034 &                         0.168 \\
Linear &                    0.150 &                    2.291 &                         0.214 &                         1.105 \\
MLP    &                    0.013 &                    0.043 &                         0.050 &                         0.302 \\
XGB    &                    0.011 &                    0.044 &                         0.116 &                         0.632 \\
mean   &                    0.051 &                    0.623 &                         0.074 &                         0.404 \\
\bottomrule
\end{tabular}



\begin{flushleft}
Table notes: performance based on backtesting metrics
\end{flushleft}
\label{table1}
% \end{sidewaystable}
%\end{adjustwidth}
\end{table}

\subsection*{Trading Strategies Evaluations}
Utilizing numerical predicted returns obtained from various machine learning models, we implemented both the Constrained Mean-Variance Optimization (C-MVO) strategy under the mean-variance framework and the widely-used long-short strategy. For detailed descriptions of these strategies, refer to the previous sections. Table.\ref{summary} presents the back-testing performances  for both the C-MVO and long-short strategies. The C-MVO strategy's average information ratio of 0.623 indicates robust back-testing performance, whereas the long-short strategy's average ratio of 0.404 suggests acceptable performance. These evidences further support the solid informative power in VIX Cmfs term structure. Meanwhile, according to table\ref{strategy}, the information ratios for ALSTM (0.744), LGB (0.327), LSTM (0.671), and Linear (2.291) models each surpass the corresponding performances under the long-short strategy. A plausible explanation for this discrepancy is that the C-MVO strategy more effectively leverages the direct numerical predictions for all six CMFs' returns, whereas the long-short strategy, relying solely on ordinal rankings derived from these predictions, may lose critical information like the exact magnitude relationships among the CMFs' predicted returns.

\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Statistic Metric}
% \begin{sidewaystable}
\begin{tabular}{lp{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
{} &  C-MVO Annualized Return &  C-MVO Information Ratio &  Long Short Annualized Return &  Long Short Information Ratio \\
model  &                          &                          &                               &                               \\
\midrule
ALSTM  &                    0.071 &                    0.744 &                         0.112 &                         0.579 \\
GRU    &                    0.028 &                    0.241 &                         0.080 &                         0.409 \\
LGB    &                    0.023 &                    0.327 &                        -0.085 &                        -0.367 \\
LSTM   &                    0.059 &                    0.671 &                         0.034 &                         0.168 \\
Linear &                    0.150 &                    2.291 &                         0.214 &                         1.105 \\
MLP    &                    0.013 &                    0.043 &                         0.050 &                         0.302 \\
XGB    &                    0.011 &                    0.044 &                         0.116 &                         0.632 \\
\hline
mean   &                    0.051 &                    0.623 &                         0.074 &                         0.404 \\
\bottomrule
\end{tabular}
\begin{flushleft}
Table notes: performance of backtest metric
\end{flushleft}
\label{strategy}
% \end{sidewaystable}
%\end{adjustwidth}
\end{table}
\nolinenumbers



\subsection*{Models Evaluations}
For evaluations of all machine learning models, we first separate models into three category: RNN-based model, Tree-based model and vanilla models include Linear and MLP.
Intially, the RNN-based Models, ALSTM Achieved an IC of 0.029 and an ICIR of 0.042. Ranked at 0.026 for IC and 0.041 for ICIR. Displays a favorable performance with a C-MVO Annualized Return of 0.071 and a C-MVO Information Ratio of 0.744. In terms of Long-Short strategies, it achieves a Long Short Annualized Return of 0.112 and a Long Short Information Ratio of 0.579.
GRU: Demonstrated an IC of 0.004 and an ICIR of 0.006. Ranked at 0.002 for IC and 0.004 for ICIR. Demonstrates a respectable performance, though slightly lower than ALSTM, with a C-MVO Annualized Return of 0.028 and a C-MVO Information Ratio of 0.241. In Long-Short strategies, it achieves a Long Short Annualized Return of 0.080 and a Long Short Information Ratio of 0.409.
LSTM: Showcased an IC of 0.021 and an ICIR of 0.031. Ranked at 0.016 for IC and 0.026 for ICIR. Shows competitive results with a C-MVO Annualized Return of 0.059 and a C-MVO Information Ratio of 0.671. However, its Long Short Annualized Return is relatively lower at 0.034, with a Long Short Information Ratio of 0.168.
\\Tree-based Models:
\\LGB (LightGBM): Outperformed with an IC of 0.035 and an ICIR of 0.066. Achieved a Rank IC of 0.054 and a Rank ICIR of 0.079. Outperforms in C-MVO Annualized Return and C-MVO Information Ratio with specific values missing.
XGB (XGBoost): Produced an IC of 0.047 and an ICIR of 0.071. Ranked at 0.043 for IC and 0.069 for ICIR.

Among the RNN-based models, ALSTM performed the best in terms of both IC and ICIR, and tends to have the highest returns and information ratios across both C-MVO and Long-Short strategies, making it a potentially strong performer. It may cause by the attention layer design in ALSTM, it can catch the information between $V_1-V_6$.
In the tree-based category, LGB and XGB demonstrated competitive performance, with a relative high performance.
Among all models, Linear exhibited superior performance compared to MLP. It may conclude term structure features contain important information about prediction, complex models can easily go to overfitting.

\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Model Contrast Through Statistic Metric}
\label{Model Contrast Through Statistic Metric}
% \begin{sidewaystable}
\begin{tabular}{llrrrr}
\toprule
        &     &     IC &   ICIR &  Rank IC &  Rank ICIR \\
model\_type & model &        &        &          &            \\
\midrule
RNN\_based & ALSTM &  0.029 &  0.042 &    0.026 &      0.041 \\
        & GRU &  0.004 &  0.006 &    0.002 &      0.004 \\
        & LSTM &  0.021 &  0.031 &    0.016 &      0.026 \\
Tree\_based & LGB &  0.035 &  0.066 &    0.054 &      0.079 \\
        & XGB &  0.047 &  0.071 &    0.043 &      0.069 \\
Vanilla & Linear &  0.114 &  0.162 &    0.091 &      0.152 \\
        & MLP &  0.011 &  0.017 &    0.012 &      0.020 \\
\bottomrule
\end{tabular}




\begin{flushleft}
Table notes: performance of backtest metric
\end{flushleft}
\label{table1}
% \end{sidewaystable}
%\end{adjustwidth}

\end{table}


\begin{table}[!ht]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{Model Contrast Through Backtest Metric}
% \begin{sidewaystable}
\begin{tabular}{llp{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
        &     &  C-MVO Annualized Return &  C-MVO Information Ratio &  Long Short Annualized Return &  Long Short Information Ratio \\
model\_type & model &                          &                          &                               &                               \\
\midrule
RNN\_based & ALSTM &                    0.071 &                    0.744 &                         0.112 &                         0.579 \\
        & GRU &                    0.028 &                    0.241 &                         0.080 &                         0.409 \\
        & LSTM &                    0.059 &                    0.671 &                         0.034 &                         0.168 \\
Tree\_based & LGB &                    0.023 &                    0.327 &                        -0.085 &                        -0.367 \\
        & XGB &                    0.011 &                    0.044 &                         0.116 &                         0.632 \\
Vanilla & Linear &                    0.150 &                    2.291 &                         0.214 &                         1.105 \\
        & MLP &                    0.013 &                    0.043 &                         0.050 &                         0.302 \\
\bottomrule
\end{tabular}




\begin{flushleft}
Table notes: performance of backtest metric
\end{flushleft}
\label{table1}
% \end{sidewaystable}
%\end{adjustwidth}

\end{table}



% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for
% step-by-step instructions.
%
\subsubsection*{End notes}
In our experiment, we also employ various innovative and complex models such as transformer, SFM, graph attention networks, and tabnet. However, these models are subjected to overfitting caused by insufficient daily-frequency data, thereby leading to bad predictive performances. Therefore, we decide not to include these models in the main part of our paper, the related performances are included in the appendix.

\begin{thebibliography}{10}

\bibitem{bib1}
Conant GC, Wolfe KH.
\newblock {{T}urning a hobby into a job: how duplicated genes find new
  functions}.
\newblock Nat Rev Genet. 2008 Dec;9(12):938--950.

\bibitem{bib2}
Ohno S.
\newblock Evolution by gene duplication.
\newblock London: George Alien \& Unwin Ltd. Berlin, Heidelberg and New York:
  Springer-Verlag.; 1970.

\bibitem{bib3}
Magwire MM, Bayer F, Webster CL, Cao C, Jiggins FM.
\newblock {{S}uccessive increases in the resistance of {D}rosophila to viral
  infection through a transposon insertion followed by a {D}uplication}.
\newblock PLoS Genet. 2011 Oct;7(10):e1002337.

\end{thebibliography}


\end{document}

